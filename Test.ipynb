{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled22.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "okoTd0rHj1hC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !mkdir ./detections/\n",
        "# !wget -O ./detections/detections.zip http://jacarini.dinf.usherbrooke.ca/static/pedestrian%20detection//pedestrian%20detection%20dataset.zip\n",
        "# !unzip ./detections/detections.zip -d ./detections/\n",
        "# !rm ./detections/detections.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc9BQPqxj7FT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files = {}\n",
        "aa = lambda x, y: list(range(x, y))\n",
        "files[\"backdoor\"] = aa(57, 2000)\n",
        "files[\"pedestrians\"] = aa(0, 1090)\n",
        "files[\"PETS2006\"] = aa(0, 1200)\n",
        "files[\"peopleInShade\"] = aa(0, 1190)\n",
        "\n",
        "gt = {}\n",
        "gt[\"backdoor\"] = [[] for i in range(0, 2000)]\n",
        "gt[\"pedestrians\"] = [[] for i in range(0, 1090)]\n",
        "gt[\"PETS2006\"] = [[] for i in range(0, 1200)]\n",
        "gt[\"peopleInShade\"] = [[] for i in range(0, 1190)]\n",
        "\n",
        "start = {\"backdoor\": [0], \n",
        "         \"pedestrians\": [0],\n",
        "         \"PETS2006\": [0],\n",
        "         \"peopleInShade\": [0]}\n",
        "\n",
        "# files = {}\n",
        "# aa = lambda x, y: list(range(x, y))\n",
        "\n",
        "# # files[\"skating\"] = aa(0, 3900)\n",
        "# # files[\"sofa\"] = aa(0, 2750)\n",
        "# files[\"busStation\"] = aa(0, 1250)\n",
        "\n",
        "# gt = {}\n",
        "# # gt[\"skating\"] = [[] for i in range(0, 3901)]\n",
        "# # gt[\"sofa\"] = [[] for i in range(0, 2751)]\n",
        "# gt[\"busStation\"] = [[] for i in range(0, 1251)]\n",
        "\n",
        "# start = {\"skating\": [0], \n",
        "#          \"sofa\": [0],\n",
        "#          \"busStation\": [0]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXM2YFEir2Jn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for key in gt:\n",
        "    with open(\"/content/detections/pedestrian detection dataset/\" + key + \"/gt.txt\") as f:\n",
        "        detections = f.readlines()\n",
        "        for detection in detections:\n",
        "            items = detection.split()\n",
        "            if len(items) == 0:\n",
        "                continue\n",
        "            frameID = int(items[0])\n",
        "            if frameID < start[key][0]:\n",
        "                continue\n",
        "            x0 = int(float(items[2]))\n",
        "            y0 = int(float(items[3]))\n",
        "            w = int(float(items[4]))\n",
        "            if frameID == 445 and key == \"PETS2006\":\n",
        "                h = 82\n",
        "            elif frameID == 938 and key == \"sofa\":\n",
        "                h = 126\n",
        "            else:\n",
        "                h = int(float(items[5]))\n",
        "            gt[key][frameID-start[key][0]].append([x0, y0, x0+w, y0+h])\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb7lhLrCr3dI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import numpy as np\n",
        "\n",
        "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
        "from torchvision.models.detection.generalized_rcnn import GeneralizedRCNN\n",
        "\n",
        "from collections import OrderedDict\n",
        "from torch import nn\n",
        "import warnings\n",
        "from torch.jit.annotations import Tuple, List, Dict, Optional\n",
        "from torch import Tensor\n",
        "from copy import deepcopy\n",
        "from torchvision.utils import save_image\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlWvdXLiF_OP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NetA = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False) \n",
        "# NetA.transform = GeneralizedRCNNTransform(min_size=400,\n",
        "#                                      max_size=800,\n",
        "#                                      image_mean = [0.485, 0.456, 0.406],\n",
        "#                                      image_std = [0.229, 0.224, 0.225])\n",
        "# NetA.roi_heads.box_predictor.cls_score = nn.Linear(in_features=1024, out_features=2, bias=True)\n",
        "# NetA.roi_heads.box_predictor.bbox_pred = nn.Linear(in_features=1024, out_features=8, bias=True)\n",
        "\n",
        "# NetA.load_state_dict(torch.load(\"/content/drive/My Drive/889PR/bestA.pth\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3H0aOluGA2W",
        "colab_type": "code",
        "outputId": "252365e2-ed09-44f5-8311-127be4b2ad1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "class NETB(GeneralizedRCNN):\n",
        "    def __init__(self, backbone, rpn, roi_heads, transform):\n",
        "        super().__init__(backbone=backbone, \n",
        "                         rpn=rpn, \n",
        "                         roi_heads=roi_heads, \n",
        "                         transform=transform)\n",
        "    \n",
        "    def forward(self, xs, ys, targets=None):\n",
        "        if self.training and targets is None:\n",
        "            raise ValueError(\"In training mode, targets should be passed\")\n",
        "        original_image_sizes = torch.jit.annotate(List[Tuple[int, int]], [])\n",
        "        for img in xs:\n",
        "            val = img.shape[-2:]\n",
        "            assert len(val) == 2\n",
        "            original_image_sizes.append((val[0], val[1]))\n",
        "\n",
        "        images = []\n",
        "        for i in range(len(xs)):\n",
        "            images.append(torch.cat((xs[i], ys[i]), dim=0))\n",
        "        images, targets = self.transform(images, targets)\n",
        "        \n",
        "        features = self.backbone(images.tensors)\n",
        "        if isinstance(features, torch.Tensor):\n",
        "            features = OrderedDict([('0', features)])\n",
        "        \n",
        "        proposals, proposal_losses = self.rpn(images, features, targets)\n",
        "        detections, detector_losses = self.roi_heads(features, proposals, images.image_sizes, targets)\n",
        "        detections = self.transform.postprocess(detections, images.image_sizes, original_image_sizes)\n",
        "\n",
        "        losses = {}\n",
        "        losses.update(detector_losses)\n",
        "        losses.update(proposal_losses)\n",
        "\n",
        "        if torch.jit.is_scripting():\n",
        "            if not self._has_warned:\n",
        "                warnings.warn(\"RCNN always returns a (Losses, Detections) tuple in scripting\")\n",
        "                self._has_warned = True\n",
        "            return (losses, detections)\n",
        "        else:\n",
        "            return self.eager_outputs(losses, detections)\n",
        "\n",
        "NetB = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)   \n",
        "backbone = NetB.backbone\n",
        "transform = GeneralizedRCNNTransform(min_size=400,\n",
        "                                     max_size=800,\n",
        "                                     image_mean = [0.485, 0.456, 0.406, 0.485, 0.456, 0.406],\n",
        "                                     image_std = [0.229, 0.224, 0.225, 0.229, 0.224, 0.225])\n",
        "rpn = NetB.rpn\n",
        "roi_heads = NetB.roi_heads\n",
        "backbone.body.conv1 = nn.Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "roi_heads.box_predictor.cls_score = nn.Linear(in_features=1024, out_features=2, bias=True)\n",
        "roi_heads.box_predictor.bbox_pred = nn.Linear(in_features=1024, out_features=8, bias=True)\n",
        "\n",
        "NetB = NETB(backbone=backbone, rpn=rpn, roi_heads=roi_heads, transform=transform)\n",
        "NetB.load_state_dict(torch.load(\"/content/drive/My Drive/889PR/bestBrandom.pth\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzmyQDUAV9Zl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from collections import OrderedDict as OD\n",
        "# class NETC(GeneralizedRCNN):\n",
        "#     def __init__(self, backbone, rpn, roi_heads, transform, STEPS):\n",
        "#         super().__init__(backbone=backbone, \n",
        "#                          rpn=rpn, \n",
        "#                          roi_heads=roi_heads, \n",
        "#                          transform=transform)\n",
        "#         self.MergeNet = nn.Sequential(nn.Conv2d(2*STEPS, 2*STEPS, 1, bias=False),\n",
        "#                                       nn.BatchNorm2d(2*STEPS, affine=False),\n",
        "#                                       nn.ReLU(inplace=True),\n",
        "#                                       nn.Conv2d(2*STEPS, STEPS, 1, bias=False),\n",
        "#                                       nn.BatchNorm2d(STEPS, affine=False),\n",
        "#                                       nn.ReLU(inplace=True))\n",
        "#         self.STEPS = STEPS\n",
        "#     def forward(self, xs, ys, targets=None):\n",
        "#         STEPS = self.STEPS\n",
        "#         if self.training and targets is None:\n",
        "#             raise ValueError(\"In training mode, targets should be passed\")\n",
        "#         original_image_sizes = torch.jit.annotate(List[Tuple[int, int]], [])\n",
        "#         for img in xs:\n",
        "#             val = img.shape[-2:]\n",
        "#             assert len(val) == 2\n",
        "#             original_image_sizes.append((val[0], val[1]))\n",
        "\n",
        "#         xs, targets = self.transform(xs, targets)\n",
        "#         ys, _ = self.transform(ys, None)\n",
        "        \n",
        "#         features1 = self.backbone(xs.tensors)\n",
        "#         features2 = self.backbone(ys.tensors)\n",
        "#         features3 = OD()\n",
        "#         for key in features1:\n",
        "#             features3[key] = torch.zeros(features1[key].size()).cuda()\n",
        "\n",
        "#         for key in features3:\n",
        "#             for i in range(features3[key].size(0)):\n",
        "#                 for j in range(0, features3[key].size(1), STEPS):\n",
        "#                     inp = torch.zeros(1, 2 * STEPS, features1[key].size(2), features1[key].size(3))\n",
        "#                     inp[0, 0 : 2 * STEPS : 2, :, :] = features1[key][i, j:j+STEPS, :, :].unsqueeze(0)\n",
        "#                     inp[0, 1 : 2 * STEPS : 2, :, :] = features2[key][i, j:j+STEPS, :, :].unsqueeze(0)\n",
        "#                     features3[key][i,j:j+STEPS,:,:] = self.MergeNet(inp.cuda()).unsqueeze(0)       \n",
        "        \n",
        "#         proposals, proposal_losses = self.rpn(xs, features3, targets)\n",
        "#         detections, detector_losses = self.roi_heads(features3, proposals, xs.image_sizes, targets)\n",
        "#         detections = self.transform.postprocess(detections, xs.image_sizes, original_image_sizes)\n",
        "\n",
        "#         losses = {}\n",
        "#         losses.update(detector_losses)\n",
        "#         losses.update(proposal_losses)\n",
        "\n",
        "#         if torch.jit.is_scripting():\n",
        "#             if not self._has_warned:\n",
        "#                 warnings.warn(\"RCNN always returns a (Losses, Detections) tuple in scripting\")\n",
        "#                 self._has_warned = True\n",
        "#             return (losses, detections)\n",
        "#         else:\n",
        "#             return self.eager_outputs(losses, detections)\n",
        "# NetC = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)   \n",
        "# backbone = NetC.backbone\n",
        "# transform = GeneralizedRCNNTransform(min_size=400,\n",
        "#                                      max_size=800,\n",
        "#                                      image_mean = [0.485, 0.456, 0.406],\n",
        "#                                      image_std = [0.229, 0.224, 0.225])\n",
        "# rpn = NetC.rpn\n",
        "# roi_heads = NetC.roi_heads\n",
        "# roi_heads.box_predictor.cls_score = nn.Linear(in_features=1024, out_features=2, bias=True)\n",
        "# roi_heads.box_predictor.bbox_pred = nn.Linear(in_features=1024, out_features=8, bias=True)\n",
        "# steps = 64\n",
        "# NetC = NETC(backbone=backbone, rpn=rpn, roi_heads=roi_heads, transform=transform, STEPS=steps)\n",
        "# pth = \"/content/drive/My Drive/889PR/bestCrandom\" + str(2*steps) +  \"to\" + str(steps) + \".pth\"\n",
        "# NetC.load_state_dict(torch.load(pth))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6KRWSeQjGf0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from collections import OrderedDict as OD\n",
        "# class NETD(GeneralizedRCNN):\n",
        "#     def __init__(self, backbone, rpn, roi_heads, transform):\n",
        "#         super().__init__(backbone=backbone, \n",
        "#                          rpn=rpn, \n",
        "#                          roi_heads=roi_heads, \n",
        "#                          transform=transform)\n",
        "    \n",
        "#     def forward(self, xs, ys, targets=None):\n",
        "#         if self.training and targets is None:\n",
        "#             raise ValueError(\"In training mode, targets should be passed\")\n",
        "#         original_image_sizes = torch.jit.annotate(List[Tuple[int, int]], [])\n",
        "#         for img in xs:\n",
        "#             val = img.shape[-2:]\n",
        "#             assert len(val) == 2\n",
        "#             original_image_sizes.append((val[0], val[1]))\n",
        "\n",
        "#         xs, targets = self.transform(xs, targets)\n",
        "#         ys, _ = self.transform(ys, None)\n",
        "        \n",
        "#         features1 = self.backbone(xs.tensors)\n",
        "#         features2 = self.backbone(ys.tensors)\n",
        "#         features3 = OD()\n",
        "#         for key in features1:\n",
        "#             features3[key] = torch.max(features1[key], features2[key])\n",
        "        \n",
        "#         proposals, proposal_losses = self.rpn(xs, features3, targets)\n",
        "#         detections, detector_losses = self.roi_heads(features3, proposals, xs.image_sizes, targets)\n",
        "#         detections = self.transform.postprocess(detections, xs.image_sizes, original_image_sizes)\n",
        "\n",
        "#         losses = {}\n",
        "#         losses.update(detector_losses)\n",
        "#         losses.update(proposal_losses)\n",
        "\n",
        "#         if torch.jit.is_scripting():\n",
        "#             if not self._has_warned:\n",
        "#                 warnings.warn(\"RCNN always returns a (Losses, Detections) tuple in scripting\")\n",
        "#                 self._has_warned = True\n",
        "#             return (losses, detections)\n",
        "#         else:\n",
        "#             return self.eager_outputs(losses, detections)\n",
        "\n",
        "# NetD = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)   \n",
        "# backbone = NetD.backbone\n",
        "# transform = GeneralizedRCNNTransform(min_size=300,\n",
        "#                                      max_size=800,\n",
        "#                                      image_mean = [0.485, 0.456, 0.406],\n",
        "#                                      image_std = [0.229, 0.224, 0.225])\n",
        "# rpn = NetD.rpn\n",
        "# roi_heads = NetD.roi_heads\n",
        "# roi_heads.box_predictor.cls_score = nn.Linear(in_features=1024, out_features=2, bias=True)\n",
        "# roi_heads.box_predictor.bbox_pred = nn.Linear(in_features=1024, out_features=8, bias=True)\n",
        "\n",
        "# NetD = NETD(backbone=backbone, rpn=rpn, roi_heads=roi_heads, transform=transform)\n",
        "# NetD.load_state_dict(torch.load(\"/content/drive/My Drive/889PR/bestDmax.pth\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFjlekDCvWpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def IOU(boxA, boxB):\n",
        "    l = max(boxA[0], boxB[0])\n",
        "    r = min(boxA[2], boxB[2])\n",
        "    t = max(boxA[1], boxB[1])\n",
        "    b = max(boxA[3], boxB[3])\n",
        "\n",
        "    intersection = max(r-l, 0) * max(b-t, 0)\n",
        "    union = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1]) + (boxB[2] - boxB[0]) * (boxB[3] - boxB[1]) - intersection\n",
        "    return intersection / union\n",
        "\n",
        "def SCORE(gts, dets):\n",
        "    if len(gts) == 0:\n",
        "        return 0, len(dets), 0\n",
        "    if len(dets) == 0:\n",
        "        return 0, 0, len(gts)\n",
        "    scores = np.zeros((len(dets), len(gts)))\n",
        "    mask = np.ones_like(scores)\n",
        "    for i, gt in enumerate(gts):\n",
        "        for j, det in enumerate(dets):\n",
        "            scores[j, i] = IOU(gt, det)\n",
        "    TP, FP, FN = 0, 0, 0\n",
        "    t = []\n",
        "    while np.max(mask*scores) > 0:\n",
        "        loc = np.argmax(mask * scores)\n",
        "        val = np.max(mask * scores)\n",
        "        mask[loc//mask.shape[1], :] = 0\n",
        "        mask[:, loc%mask.shape[1]] = 0\n",
        "        # print(mask)\n",
        "        if val > 0.5:\n",
        "            TP += 1\n",
        "    FN = len(gts) - TP\n",
        "    FP = len(dets) - TP\n",
        "    return TP, FP, FN\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4UHP0gH2YTD",
        "colab_type": "code",
        "outputId": "ca443f45-338a-44d7-bac0-688f7e4d50ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        }
      },
      "source": [
        "model = NetB\n",
        "batch_size = 5\n",
        "def imread(path, num):\n",
        "    im = Image.open(path + str(num).zfill(6) + \".jpg\")\n",
        "    transform = T.Compose([T.ToTensor()])\n",
        "    im = transform(im)\n",
        "    return im.cuda()\n",
        "\n",
        "model.cuda()\n",
        "model.eval()\n",
        "\n",
        "metrics = {}\n",
        "metrics[\"PETS2006\"] = {\"TP\": 0, \"FN\": [0, []], \"FP\": [0, []]}\n",
        "metrics[\"backdoor\"] = {\"TP\": 0, \"FN\": [0, []], \"FP\": [0, []]}\n",
        "metrics[\"pedestrians\"] = {\"TP\": 0, \"FN\": [0, []], \"FP\": [0, []]}\n",
        "metrics[\"peopleInShade\"] = {\"TP\": 0, \"FN\": [0, []], \"FP\": [0, []]}\n",
        "# metrics[\"skating\"] = {\"TP\": 0, \"FN\": [0, []], \"FP\": [0, []]}\n",
        "# metrics[\"sofa\"] = {\"TP\": 0, \"FN\": [0, []], \"FP\": [0, []]}\n",
        "# metrics[\"busStation\"] = {\"TP\": 0, \"FN\": [0, []], \"FP\": [0, []]}\n",
        "\n",
        "for key in files:\n",
        "    bg = imread(\"/content/detections/pedestrian detection dataset/\" + key +\"/input/in\", 1)\n",
        "    for i in range(0, len(files[key]), batch_size):\n",
        "        if i % 20 == 0:\n",
        "            save_image(bg, key + str(i) + '.png')\n",
        "        print(\"\\r %s: [%d / %d]\" % (key, i, len(files[key])), end=\"\")\n",
        "        ids = files[key][i:i+batch_size]\n",
        "        sample = [imread(\"/content/detections/pedestrian detection dataset/\" + key +\"/input/in\", num+1) for num in ids]\n",
        "        detections = model(sample, [bg]*10)\n",
        "        # detections = model(sample)\n",
        "        results = []\n",
        "        for detection in detections:\n",
        "            result = []\n",
        "            scores = detection[\"scores\"].detach().cpu()\n",
        "            boxes = detection[\"boxes\"].detach().cpu()\n",
        "            for j, score in enumerate(scores):\n",
        "                if score > 0.5:\n",
        "                    result.append(boxes[j].numpy())\n",
        "            results.append(result)\n",
        "        mask = torch.zeros(bg.size()).cuda()\n",
        "        \n",
        "        if len(results[-1]) > 0:\n",
        "            result = results[-1]\n",
        "            for re in result:\n",
        "                x0 = re[0]\n",
        "                y0 = re[1]\n",
        "                x1 = re[2]\n",
        "                y1 = re[3]\n",
        "                mask[:, int(x0):int(x1), int(y0):int(y1)] = 1\n",
        "        bg = 1-mask * bg + (1-mask) * sample[-1]\n",
        "        gts = gt[key][i:i+batch_size]\n",
        "        for j, gtt in enumerate(gts):\n",
        "            tp, fp, fn = SCORE(gtt, results[j])\n",
        "            metrics[key][\"TP\"] +=  tp\n",
        "            metrics[key][\"FP\"][0] +=  fp\n",
        "            metrics[key][\"FN\"][0] +=  fn\n",
        "            if fp > 0:\n",
        "                metrics[key][\"FP\"][1].append(ids[j])\n",
        "            if fn > 0:\n",
        "                metrics[key][\"FN\"][1].append(ids[j])\n",
        "    print(\"\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r backdoor: [0 / 1943]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " backdoor: [165 / 1943]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-92b86530ec8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/detections/pedestrian detection dataset/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\"/input/in\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;31m# detections = model(sample)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-d632e776d173>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xs, ys, targets)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0mobjectness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_bbox_deltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m         \u001b[0manchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0mnum_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image_list, feature_maps)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         strides = [[torch.tensor(image_size[0] / g[0], dtype=torch.int64, device=device),\n\u001b[0;32m--> 164\u001b[0;31m                     torch.tensor(image_size[1] / g[1], dtype=torch.int64, device=device)] for g in grid_sizes]\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_cell_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0manchors_over_all_feature_maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcached_grid_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         strides = [[torch.tensor(image_size[0] / g[0], dtype=torch.int64, device=device),\n\u001b[0;32m--> 164\u001b[0;31m                     torch.tensor(image_size[1] / g[1], dtype=torch.int64, device=device)] for g in grid_sizes]\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_cell_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0manchors_over_all_feature_maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcached_grid_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZmDT-RpKPx_",
        "colab_type": "code",
        "outputId": "fd6b4523-7938-4989-e022-90ebadd0a775",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "bg.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 240, 320])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asknLpP44XiA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename = 'NetC8b.txt'\n",
        "# filename = 'NetDb.txt'\n",
        "for key in metrics:\n",
        "    with open(filename, 'a') as f:\n",
        "        print(key, file=f)\n",
        "    with open(filename, 'a') as f:\n",
        "        print(\"TP:\", metrics[key][\"TP\"], file=f)\n",
        "    with open(filename, 'a') as f:\n",
        "        print(\"FP:\", metrics[key][\"FP\"][0], file=f)\n",
        "    with open(filename, 'a') as f:\n",
        "        print(\"FN:\", metrics[key][\"FN\"][0], file=f)\n",
        "    with open(filename, 'a') as f:\n",
        "        print(\"FP:\", metrics[key][\"FP\"][1], file=f)\n",
        "    with open(filename, 'a') as f:\n",
        "        print(\"FN:\", metrics[key][\"FN\"][1], file=f)\n",
        "    with open(filename, 'a') as f:   \n",
        "        print(\"--\" * 20, file=f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er7Umqc-EL1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2q2ezJvHj5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tcB0TEVH75Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}