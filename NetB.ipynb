{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNet6432.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "40d9fad4b2e7408091c4323eafc55779": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d2dcdc6095cb4250a097c1b9de6ed856",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5aa2108319d1408e93209d5f8a4d322b",
              "IPY_MODEL_af569717efcc412489c293bcf8e6e25b"
            ]
          }
        },
        "d2dcdc6095cb4250a097c1b9de6ed856": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5aa2108319d1408e93209d5f8a4d322b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_72837d226e4747019c6fe35fdf30d2a4",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 167502836,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 167502836,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6c27b50db4ab4765b6f316e5bb6f4010"
          }
        },
        "af569717efcc412489c293bcf8e6e25b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a4a0c490d52841f3b95252cd9660a998",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 160M/160M [00:00&lt;00:00, 236MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8429329411c34725a53111e1556b973e"
          }
        },
        "72837d226e4747019c6fe35fdf30d2a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6c27b50db4ab4765b6f316e5bb6f4010": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a4a0c490d52841f3b95252cd9660a998": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8429329411c34725a53111e1556b973e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz0uPe6I6zUS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -O backdoor.zip http://jacarini.dinf.usherbrooke.ca/static/dataset/shadow/backdoor.zip\n",
        "!wget -O pedestrian.zip http://jacarini.dinf.usherbrooke.ca/static/dataset/baseline/pedestrians.zip\n",
        "!wget -O cubicle.zip http://jacarini.dinf.usherbrooke.ca/static/dataset/shadow/cubicle.zip\n",
        "!wget -O busStation.zip http://jacarini.dinf.usherbrooke.ca/static/dataset/shadow/busStation.zip\n",
        "!wget -O peopleinshadow.zip http://jacarini.dinf.usherbrooke.ca/static/dataset/shadow/peopleInShade.zip\n",
        "\n",
        "!unzip backdoor.zip\n",
        "!unzip pedestrian.zip\n",
        "!unzip cubicle.zip\n",
        "!unzip busStation.zip\n",
        "!unzip peopleinshadow.zip\n",
        "\n",
        "!rm backdoor.zip\n",
        "!rm pedestrian.zip\n",
        "!rm cubicle.zip\n",
        "!rm busStation.zip\n",
        "!rm peopleinshadow.zip\n",
        "\n",
        "!mkdir ./detections/\n",
        "!wget -O ./detections/detections.zip http://jacarini.dinf.usherbrooke.ca/static/pedestrian%20detection//pedestrian%20detection%20dataset.zip\n",
        "!unzip ./detections/detections.zip -d ./detections/\n",
        "!rm ./detections/detections.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7q2_bx--65OL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files = {}\n",
        "aa = lambda x, y: list(range(x, y))\n",
        "files[\"backdoor\"] = {\"background\": [(400, 1358), (1561, 1629), (1995, 2000)],\n",
        "                     \"foreground\": [aa(1404, 1524), \n",
        "                                    aa(1658, 1742), \n",
        "                                    aa(1833, 1870) + aa(1897, 1929)]}\n",
        "files[\"cubicle\"] = {\"background\": [(1416, 1557), (1817, 1951), (6641, 6924), (7181, 7400)],\n",
        "                    \"foreground\": [aa(1172, 1398),\n",
        "                                   aa(1611, 1801),\n",
        "                                   aa(1972, 2174),\n",
        "                                   aa(2308, 2522),\n",
        "                                   aa(6035, 6152),\n",
        "                                   aa(6323, 6453),\n",
        "                                   aa(6500, 6571),\n",
        "                                   aa(6601, 6629),\n",
        "                                   aa(6934, 7121)]}\n",
        "files[\"pedestrians\"] = {\"background\": [(713, 826)], \n",
        "                        \"foreground\": [aa(318, 401),\n",
        "                                       aa(511, 556), \n",
        "                                       aa(602, 694), \n",
        "                                       aa(854, 1035), \n",
        "                                       aa(1056, 1099)]}\n",
        "files[\"peopleInShade\"] = {\"background\":[(250, 284), (371, 434), (509, 573), \n",
        "                                         (808, 825), (989, 1056)], \n",
        "                           \"foreground\":[aa(292, 313), \n",
        "                                         aa(451, 502), \n",
        "                                         aa(589, 796), \n",
        "                                         aa(845, 858)]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSoImaOZ8UJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import random\n",
        "\n",
        "savepath = \"./dataset/\"\n",
        "readpath = \"./detections/pedestrian detection dataset/\"\n",
        "if not os.path.isdir(savepath):\n",
        "    os.makedirs(savepath)\n",
        "if not os.path.isdir(\"./dataset/gt/\"):\n",
        "    os.makedirs(\"./dataset/gt/\")\n",
        "if not os.path.isdir(\"./dataset/in/\"):\n",
        "    os.makedirs(\"./dataset/in/\")\n",
        "if not os.path.isdir(\"./dataset/bk/\"):\n",
        "    os.makedirs(\"./dataset/bk/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuFBVuT28WUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def syn(persons, gts, back, overlap, seed):\n",
        "    finalgt = np.zeros_like(gts[0])\n",
        "    finalim = back.copy()\n",
        "    orders = list(range(len(persons)))\n",
        "    random.seed(seed)\n",
        "    random.shuffle(orders)\n",
        "    xywh = []\n",
        "    seedi = seed + 119\n",
        "    seedj = seed + 2843\n",
        "\n",
        "    for i, order in enumerate(orders):\n",
        "        m = 0\n",
        "        foreground = persons[order]\n",
        "        groundtruth = gts[order]\n",
        "            \n",
        "        pixels = np.where(groundtruth[:,:,0] >160)\n",
        "        ymin = np.min(pixels[0])\n",
        "        ymax = np.max(pixels[0])\n",
        "        xmin = np.min(pixels[1])\n",
        "        xmax = np.max(pixels[1])\n",
        "        \n",
        "        newgt = groundtruth[ymin:ymax, xmin:xmax,:]\n",
        "        newgt[newgt < 200] = 0\n",
        "        newgt[newgt > 200] = 1\n",
        "        newim = foreground[ymin:ymax, xmin:xmax,:]\n",
        "        \n",
        "        random.seed(seed + 3)\n",
        "        newsize = random.gauss(0.9, 0.1)\n",
        "        newsize = 1 if newsize > 1 else newsize\n",
        "        newsize = 0.7 if newsize < 0.7 else newsize\n",
        "        newim = cv.resize(newim, None, fx=newsize, fy=newsize)\n",
        "        newgt = cv.resize(newgt, None, fx=newsize, fy=newsize)\n",
        "\n",
        "        random.seed(seed + 175)\n",
        "        if random.uniform(0, 1)>0.5:\n",
        "            newim = newim[:, ::-1, :]\n",
        "            newgt = newgt[:, ::-1, :]\n",
        "        \n",
        "        if overlap and i==1:\n",
        "            while True:\n",
        "                seedi += 1\n",
        "                m += 1\n",
        "                n = 0\n",
        "                if m > 1000:\n",
        "                    return finalim, finalgt\n",
        "                x0, y0, w0, h0 = xywh[0]\n",
        "                top = y0 - h0 * 3 // 2\n",
        "                left = x0 - w0 * 3 // 2\n",
        "                right = x0 + w0 * 3 // 2\n",
        "                bottom = y0 + h0 * 3 // 2\n",
        "                top = 0 if top < 0 else top\n",
        "                left = 0 if left < 0 else left\n",
        "                right = back.shape[1] if right > back.shape[1] else right\n",
        "                bottom = back.shape[0] if bottom > back.shape[0] else bottom\n",
        "                while True:\n",
        "                    n += 1\n",
        "                    if n > 2000:\n",
        "                        return finalim, finalgt\n",
        "                    seedj += 98\n",
        "                    random.seed(seedi + seedj)\n",
        "                    y= random.randint(top, bottom)\n",
        "                    random.seed((seedi + seedj) //32 )\n",
        "                    x = random.randint(left, right)\n",
        "\n",
        "                    if y+newim.shape[0] < back.shape[0] and x+newim.shape[1] < back.shape[1]:\n",
        "                        break\n",
        "                gtt = np.zeros_like(groundtruth)\n",
        "                gtt[y:y+newim.shape[0], x:x+newgt.shape[1]] = newgt\n",
        "                n2and1 = len(np.where((gtt * finalgt) > 0)[0])\n",
        "                n1 = len(np.where(gtt > 0)[0])\n",
        "                n2 = len(np.where(gtt > 0)[0])\n",
        "                if n2and1 < min(n1, n2) * 0.7 and n2and1 > min(n1, n2) * 0.2:\n",
        "                    break\n",
        "        else:\n",
        "            while True:\n",
        "                seedi += 1\n",
        "                m += 1\n",
        "                n = 0\n",
        "                if m > 1000:\n",
        "                    return finalim, finalgt\n",
        "                while True:\n",
        "                    seedj += 1\n",
        "                    n += 1\n",
        "                    if n > 2000:\n",
        "                        return finalim, finalgt\n",
        "                    random.seed(seedi + seedj + 21)\n",
        "                    y = random.randint(0, back.shape[0])\n",
        "                    random.seed(seedi + seedj + 981)\n",
        "                    x = random.randint(0, back.shape[1])\n",
        "                    if y+newim.shape[0] < back.shape[0] and x+newim.shape[1] < back.shape[1]:\n",
        "                        break\n",
        "                gtt = np.zeros_like(groundtruth)\n",
        "                gtt[y:y+newim.shape[0], x:x+newgt.shape[1]] = newgt\n",
        "                n2and1 = len(np.where((gtt * finalgt) > 0)[0])\n",
        "                n1 = len(np.where(gtt > 0)[0])\n",
        "                n2 = len(np.where(gtt > 0)[0])\n",
        "                if n2and1 < min(n1, n2) * 0.5:\n",
        "                    break\n",
        "        xywh.append((x, y, newgt.shape[1], newgt.shape[0]))\n",
        "        imm = np.zeros_like(groundtruth)\n",
        "        imm[y:y+newim.shape[0], x:x+newim.shape[1]] = newim\n",
        "\n",
        "        finalgt[gtt!=0] = 1 + order\n",
        "        finalim = finalim * (1-gtt) + imm * (gtt)\n",
        "        \n",
        "    return finalim, finalgt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KheRifQ8Yda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loadimage(path, mode, i):\n",
        "    middle = \"/groundtruth/\" if mode == \"gt\" else \"/input/\"\n",
        "    prefix = \".png\" if mode == \"gt\" else \".jpg\"\n",
        "    filename = path + middle + mode + str(i).zfill(6) + prefix\n",
        "    # print(filename)\n",
        "    im = cv.imread(filename)\n",
        "    im = cv.cvtColor(im, cv.COLOR_BGR2RGB)\n",
        "    return im"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "od8MtCCm8aGm",
        "colab_type": "code",
        "outputId": "31615b9f-bf5e-4399-f5bf-23e779d9dedd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "import torch \n",
        "\n",
        "n_frames = 3000\n",
        "max_person = 3\n",
        "\n",
        "targets = [None] * n_frames\n",
        "\n",
        "for i in range(n_frames):\n",
        "    if i % 100 == 0:\n",
        "        print(i)\n",
        "    random.seed(i+1201)\n",
        "    n_person = random.randint(2, max_person)\n",
        "    random.seed(i*72)\n",
        "    overlap = False if random.uniform(0, 1) < 0.7 else True\n",
        "    overlap = False if n_person < 2 else overlap\n",
        "    random.seed(i)\n",
        "    f = i // (n_frames // 4)\n",
        "    f = list(files.keys())[f]\n",
        "    background = files[f][\"background\"]\n",
        "    random.seed(i + 192)\n",
        "    background = background[random.randint(0, len(background)-1)]\n",
        "    random.seed(i + 212)\n",
        "    background = random.randint(background[0], background[1] - 1)\n",
        "    background = loadimage(f, 'in', background)\n",
        "    foreground = files[f][\"foreground\"]\n",
        "\n",
        "    used = []\n",
        "    prs = []\n",
        "    gts = []\n",
        "    for j in range(0, n_person):\n",
        "        seedj = 0\n",
        "        while True:\n",
        "            seedj += 1\n",
        "            random.seed(seedj + 1982)\n",
        "            foreGroup = random.randint(0, len(foreground) - 1)\n",
        "            if foreGroup not in used:\n",
        "                break\n",
        "        used.append(foreGroup)\n",
        "        random.seed(i + 2763)\n",
        "        fore = random.randint(0, len(foreground[foreGroup]) - 1)\n",
        "        fore = foreground[foreGroup][fore]\n",
        "        person = loadimage(f, 'in', fore)\n",
        "        ground = loadimage(f, 'gt', fore)\n",
        "        ground[ground<160] = 0\n",
        "        retval, labels = cv.connectedComponents(ground[:,:,0])\n",
        "        \n",
        "        for k in range(0, retval):\n",
        "            im = np.zeros((labels.shape[0], labels.shape[1], 3))\n",
        "            mask = np.zeros_like(ground)\n",
        "            mask[ground>200] = 1\n",
        "            im[labels == k, :] = 1\n",
        "            im[labels != k, :] = 0\n",
        "            im[im>0] = ground[im>0]\n",
        "            im = im * mask\n",
        "            if np.max(im) > 0:\n",
        "                prs.append(person)\n",
        "                gts.append(im)\n",
        "        \n",
        "    a = aa(0, len(prs))\n",
        "    random.seed(i)\n",
        "    random.shuffle(a)\n",
        "    a = a[:n_person]\n",
        "    prs = [prs[i] for i in a]\n",
        "    gts = [gts[i] for i in a]\n",
        "    \n",
        "    f, g = syn(prs, gts, background, overlap, i)\n",
        "    f = cv.cvtColor(np.uint8(f), cv.COLOR_RGB2BGR)\n",
        "    boxes = []\n",
        "    obj_ids = np.unique(g)[1:]\n",
        "    for idx in obj_ids:\n",
        "        pos = np.where(g==idx)\n",
        "        xmin = np.min(pos[1])\n",
        "        xmax = np.max(pos[1])\n",
        "        ymin = np.min(pos[0])\n",
        "        ymax = np.max(pos[0])\n",
        "        boxes.append([xmin, ymin, xmax, ymax])\n",
        "    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "    labels = torch.ones((len(boxes),), dtype=torch.int64)\n",
        "    image_id = torch.tensor([i])\n",
        "    area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "    iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)\n",
        "    target = {}\n",
        "    target[\"boxes\"] = boxes\n",
        "    target[\"labels\"] = labels\n",
        "    target[\"image_id\"] = image_id\n",
        "    target[\"area\"] = area\n",
        "    target[\"iscrowd\"] = iscrowd\n",
        "    targets[i] = target\n",
        "\n",
        "    cv.imwrite(\"/content/dataset/gt/\" + str(i).zfill(6) + '.png', g)\n",
        "    cv.imwrite(\"/content/dataset/in/\" + str(i).zfill(6) + '.jpg', f)\n",
        "    cv.imwrite(\"/content/dataset/bk/\" + str(i).zfill(6) + '.jpg', background)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1100\n",
            "1200\n",
            "1300\n",
            "1400\n",
            "1500\n",
            "1600\n",
            "1700\n",
            "1800\n",
            "1900\n",
            "2000\n",
            "2100\n",
            "2200\n",
            "2300\n",
            "2400\n",
            "2500\n",
            "2600\n",
            "2700\n",
            "2800\n",
            "2900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htl5cc4E8e9B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random.seed(19872)\n",
        "idx = list(range(0, n_frames))\n",
        "random.shuffle(idx)\n",
        "trainset = idx[:n_frames//6*4]\n",
        "valset = idx[n_frames//6*4:n_frames//6*5]\n",
        "testset = idx[n_frames//6*5:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcGnusQM8mwU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import numpy as np\n",
        "\n",
        "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
        "from torchvision.models.detection.generalized_rcnn import GeneralizedRCNN\n",
        "\n",
        "from collections import OrderedDict\n",
        "from torch import nn\n",
        "import warnings\n",
        "from torch.jit.annotations import Tuple, List, Dict, Optional\n",
        "from torch import Tensor\n",
        "from copy import deepcopy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHP6r2oAsC55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "STEPS = 32\n",
        "class NETC(GeneralizedRCNN):\n",
        "    def __init__(self, backbone, rpn, roi_heads, transform):\n",
        "        super().__init__(backbone=backbone, \n",
        "                         rpn=rpn, \n",
        "                         roi_heads=roi_heads, \n",
        "                         transform=transform)\n",
        "        self.MergeNet = nn.Sequential(nn.Conv2d(2*STEPS, 2*STEPS, 1, bias=False),\n",
        "                                      nn.BatchNorm2d(2*STEPS, affine=False),\n",
        "                                      nn.ReLU(inplace=True),\n",
        "                                      nn.Conv2d(2*STEPS, STEPS, 1, bias=False),\n",
        "                                      nn.BatchNorm2d(STEPS, affine=False),\n",
        "                                      nn.ReLU(inplace=True))\n",
        "    \n",
        "    def forward(self, xs, ys, targets=None):\n",
        "        if self.training and targets is None:\n",
        "            raise ValueError(\"In training mode, targets should be passed\")\n",
        "        original_image_sizes = torch.jit.annotate(List[Tuple[int, int]], [])\n",
        "        for img in xs:\n",
        "            val = img.shape[-2:]\n",
        "            assert len(val) == 2\n",
        "            original_image_sizes.append((val[0], val[1]))\n",
        "\n",
        "        xs, targets = self.transform(xs, targets)\n",
        "        ys, _ = self.transform(ys, None)\n",
        "        \n",
        "        features1 = self.backbone(xs.tensors)\n",
        "        features2 = self.backbone(ys.tensors)\n",
        "        features3 = deepcopy(features1)\n",
        "        for key in features3:\n",
        "            for i in range(features3[key].size(0)):\n",
        "                for j in range(0, features3[key].size(1), STEPS):\n",
        "                    inp = torch.zeros(1, 2 * STEPS, features1[key].size(2), features1[key].size(3))\n",
        "                    inp[0, 0 : 2 * STEPS : 2, :, :] = features1[key][i, j:j+STEPS, :, :].unsqueeze(0)\n",
        "                    inp[0, 1 : 2 * STEPS : 2, :, :] = features2[key][i, j:j+STEPS, :, :].unsqueeze(0)\n",
        "                    features3[key][i,j:j+STEPS,:,:] = self.MergeNet(inp.cuda()).unsqueeze(0)       \n",
        "        \n",
        "        proposals, proposal_losses = self.rpn(xs, features3, targets)\n",
        "        detections, detector_losses = self.roi_heads(features3, proposals, xs.image_sizes, targets)\n",
        "        detections = self.transform.postprocess(detections, xs.image_sizes, original_image_sizes)\n",
        "\n",
        "        losses = {}\n",
        "        losses.update(detector_losses)\n",
        "        losses.update(proposal_losses)\n",
        "\n",
        "        if torch.jit.is_scripting():\n",
        "            if not self._has_warned:\n",
        "                warnings.warn(\"RCNN always returns a (Losses, Detections) tuple in scripting\")\n",
        "                self._has_warned = True\n",
        "            return (losses, detections)\n",
        "        else:\n",
        "            return self.eager_outputs(losses, detections)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMX3rc0stO1x",
        "colab_type": "code",
        "outputId": "2dff11b3-c185-40fd-d08e-0cee7fc573bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "40d9fad4b2e7408091c4323eafc55779",
            "d2dcdc6095cb4250a097c1b9de6ed856",
            "5aa2108319d1408e93209d5f8a4d322b",
            "af569717efcc412489c293bcf8e6e25b",
            "72837d226e4747019c6fe35fdf30d2a4",
            "6c27b50db4ab4765b6f316e5bb6f4010",
            "a4a0c490d52841f3b95252cd9660a998",
            "8429329411c34725a53111e1556b973e"
          ]
        }
      },
      "source": [
        "NetC = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)   \n",
        "for p in NetC.parameters():\n",
        "    p.requires_grad = False\n",
        "backbone = NetC.backbone\n",
        "transform = GeneralizedRCNNTransform(min_size=400,\n",
        "                                     max_size=800,\n",
        "                                     image_mean = [0.485, 0.456, 0.406],\n",
        "                                     image_std = [0.229, 0.224, 0.225])\n",
        "rpn = NetC.rpn\n",
        "roi_heads = NetC.roi_heads\n",
        "roi_heads.box_predictor.cls_score = nn.Linear(in_features=1024, out_features=2, bias=True)\n",
        "roi_heads.box_predictor.bbox_pred = nn.Linear(in_features=1024, out_features=8, bias=True)\n",
        "\n",
        "NetC = NETC(backbone=backbone, rpn=rpn, roi_heads=roi_heads, transform=transform)\n",
        "\n",
        "for name, param in NetC.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40d9fad4b2e7408091c4323eafc55779",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=167502836.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "roi_heads.box_predictor.cls_score.weight\n",
            "roi_heads.box_predictor.cls_score.bias\n",
            "roi_heads.box_predictor.bbox_pred.weight\n",
            "roi_heads.box_predictor.bbox_pred.bias\n",
            "MergeNet.0.weight\n",
            "MergeNet.3.weight\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozY8ww2otbg5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def IOU(boxA, boxB):\n",
        "    l = max(boxA[0], boxB[0])\n",
        "    r = min(boxA[2], boxB[2])\n",
        "    t = max(boxA[1], boxB[1])\n",
        "    b = max(boxA[3], boxB[3])\n",
        "\n",
        "    intersection = max(r-l, 0) * max(b-t, 0)\n",
        "    union = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1]) + (boxB[2] - boxB[0]) * (boxB[3] - boxB[1]) - intersection\n",
        "    return intersection / union\n",
        "\n",
        "def SCORE(gts, dets):\n",
        "    if len(gts) == 0:\n",
        "        return 0, len(dets), 0\n",
        "    if len(dets) == 0:\n",
        "        return 0, 0, len(gts)\n",
        "    scores = np.zeros((len(dets), len(gts)))\n",
        "    mask = np.ones_like(scores)\n",
        "    for i, gt in enumerate(gts):\n",
        "        for j, det in enumerate(dets):\n",
        "            scores[j, i] = IOU(gt, det)\n",
        "    TP, FP, FN = 0, 0, 0\n",
        "    t = []\n",
        "    while np.max(mask*scores) > 0:\n",
        "        loc = np.argmax(mask * scores)\n",
        "        val = np.max(mask * scores)\n",
        "        mask[loc//mask.shape[1], :] = 0\n",
        "        mask[:, loc%mask.shape[1]] = 0\n",
        "        if val > 0.5:\n",
        "            TP += 1\n",
        "    FN = len(gts) - TP\n",
        "    FP = len(dets) - TP\n",
        "    return TP, FP, FN\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oNFo_TPtd9N",
        "colab_type": "code",
        "outputId": "9acab9b3-869d-4692-bb5b-3389bd76515b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "n_epochs = 20\n",
        "batch_size = 10\n",
        "def imread(path, num):\n",
        "    im = Image.open(path + str(num).zfill(6) + \".jpg\")\n",
        "    transform = T.Compose([T.ToTensor()])\n",
        "    im = transform(im)\n",
        "    return im.cuda()\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "params = [p for p in NetC.parameters() if p.requires_grad]\n",
        "\n",
        "optimizer = optim.Adam(params, lr=0.001)\n",
        "\n",
        "\n",
        "NetC.cuda()\n",
        "min_loss = np.inf\n",
        "history = {'train_loss': [],\n",
        "           'val_acc': []}\n",
        "best = 0\n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = []\n",
        "    NetC.train()\n",
        "    for i in range(0, len(trainset), batch_size):\n",
        "        print(\"\\r Epoch %d, training [%d / %d]\" % (epoch+1, i//batch_size+1, int(len(trainset)/batch_size)), end='')\n",
        "        optimizer.zero_grad()\n",
        "        ids = trainset[i:i+batch_size]\n",
        "        sample = [imread(\"/content/dataset/in/\", num) for num in ids]\n",
        "        target = [{k: v.cuda() for k, v in targets[num].items()} for num in ids]\n",
        "\n",
        "        back = []\n",
        "        for num in ids:\n",
        "            random.seed(num + epoch * 1000)\n",
        "            p = random.uniform(0, 1)\n",
        "            if p < 0.25:\n",
        "                back.append(imread(\"/content/dataset/bk/\", num))\n",
        "            elif p < 0.5:\n",
        "                random.seed(num)\n",
        "                pp = (n_frames // 4)\n",
        "                x = random.randint(pp * (num//pp), pp * (num//pp + 1)-1)\n",
        "                back.append(imread(\"/content/dataset/bk/\", x))\n",
        "            elif p < 0.75:\n",
        "                back.append(imread(\"/content/dataset/in/\", num))\n",
        "            else: \n",
        "                random.seed(num)\n",
        "                pp = (n_frames // 4)\n",
        "                x = random.randint(pp * (num//pp), pp * (num//pp + 1)-1)\n",
        "                back.append(imread(\"/content/dataset/in/\", x))\n",
        "\n",
        "        loss_dict = NetC(sample, back, target)\n",
        "        running_loss = sum([loss for loss in loss_dict.values()])\n",
        "        running_loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss.append(running_loss.item())\n",
        "    NetC.eval()\n",
        "    TP, FP, FN = 0, 0, 0\n",
        "    print(\"\")\n",
        "    for i in range(0, len(valset), batch_size):\n",
        "        print(\"\\r Epoch %d, validation [%d / %d]\" % (epoch+1, i//batch_size+1, int(len(valset)/batch_size)), end='')\n",
        "        ids = valset[i:i+batch_size]\n",
        "        sample = [imread(\"/content/dataset/in/\", num) for num in ids]\n",
        "        back = []\n",
        "        for num in ids:\n",
        "            random.seed(num)\n",
        "            p = random.uniform(0, 1)\n",
        "            if p < 0.25:\n",
        "                back.append(imread(\"/content/dataset/bk/\", num))\n",
        "            elif p < 0.5:\n",
        "                random.seed(num)\n",
        "                pp = (n_frames // 4)\n",
        "                x = random.randint(pp * (num//pp), pp * (num//pp + 1)-1)\n",
        "                back.append(imread(\"/content/dataset/bk/\", x))\n",
        "            elif p < 0.75:\n",
        "                back.append(imread(\"/content/dataset/in/\", num))\n",
        "            else: \n",
        "                random.seed(num)\n",
        "                pp = (n_frames // 4)\n",
        "                x = random.randint(pp * (num//pp), pp * (num//pp + 1)-1)\n",
        "                back.append(imread(\"/content/dataset/in/\", x))\n",
        "        target = [{k: v.numpy() for k, v in targets[num].items()} for num in ids]\n",
        "        gts = []\n",
        "        for t in target:\n",
        "            boxes = t[\"boxes\"]\n",
        "            gt = [box for box in boxes]\n",
        "            gts.append(gt)\n",
        "        detections = NetC(sample, back)\n",
        "        results = []\n",
        "        for detection in detections:\n",
        "            result = []\n",
        "            scores = detection[\"scores\"].detach().cpu()\n",
        "            boxes = detection[\"boxes\"].detach().cpu()\n",
        "            for j, score in enumerate(scores):\n",
        "                if score > 0.5:\n",
        "                    result.append(boxes[j].numpy())\n",
        "            results.append(result)\n",
        "\n",
        "        for j, gt in enumerate(gts):\n",
        "            tp, fp, fn = SCORE(gt, results[j])\n",
        "            TP += tp\n",
        "            FP += fp\n",
        "            FN += fn\n",
        "    if TP == 0 and FP == 0:\n",
        "        b = 0\n",
        "    else: \n",
        "        Recall = TP / (TP + FN)\n",
        "        Precision = TP / (TP + FP)\n",
        "        b = (2 * Precision * Recall) / (Precision + Recall)\n",
        "    a = np.mean(np.array(train_loss))\n",
        "    print(\"\")\n",
        "    print(\"Epochs: %d, train loss=%f, val acc= %f\" % (epoch+1, a, b))\n",
        "    if b > best:\n",
        "        best = b\n",
        "        print(\"==> best model (acc = {:0.6f}), saving model...\".format(b))\n",
        "        best_weights = deepcopy(NetC.state_dict())\n",
        "        torch.save(best_weights, \"/content/drive/My Drive/889PR/bestCrandom\" + str(2*STEPS) + \"to\" + str(STEPS) + \".pth\")\n",
        "    \n",
        "    history[\"train_loss\"].append(a)\n",
        "    history[\"val_acc\"].append(b)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r Epoch 1, training [1 / 200]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \"\n",
            "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
            "\tnonzero(Tensor input, *, Tensor out)\n",
            "Consider using one of the following signatures instead:\n",
            "\tnonzero(Tensor input, *, bool as_tuple)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " Epoch 1, training [200 / 200]\n",
            " Epoch 1, validation [50 / 50]\n",
            "Epochs: 1, train loss=0.357194, val acc= 0.676142\n",
            "==> best model (acc = 0.676142), saving model...\n",
            " Epoch 2, training [200 / 200]\n",
            " Epoch 2, validation [50 / 50]\n",
            "Epochs: 2, train loss=0.195159, val acc= 0.803896\n",
            "==> best model (acc = 0.803896), saving model...\n",
            " Epoch 3, training [200 / 200]\n",
            " Epoch 3, validation [50 / 50]\n",
            "Epochs: 3, train loss=0.171615, val acc= 0.828891\n",
            "==> best model (acc = 0.828891), saving model...\n",
            " Epoch 4, training [200 / 200]\n",
            " Epoch 4, validation [50 / 50]\n",
            "Epochs: 4, train loss=0.159956, val acc= 0.841263\n",
            "==> best model (acc = 0.841263), saving model...\n",
            " Epoch 5, training [200 / 200]\n",
            " Epoch 5, validation [50 / 50]\n",
            "Epochs: 5, train loss=0.151686, val acc= 0.846026\n",
            "==> best model (acc = 0.846026), saving model...\n",
            " Epoch 6, training [200 / 200]\n",
            " Epoch 6, validation [50 / 50]\n",
            "Epochs: 6, train loss=0.147479, val acc= 0.846312\n",
            "==> best model (acc = 0.846312), saving model...\n",
            " Epoch 7, training [200 / 200]\n",
            " Epoch 7, validation [50 / 50]\n",
            "Epochs: 7, train loss=0.143942, val acc= 0.848804\n",
            "==> best model (acc = 0.848804), saving model...\n",
            " Epoch 8, training [200 / 200]\n",
            " Epoch 8, validation [50 / 50]\n",
            "Epochs: 8, train loss=0.141220, val acc= 0.839357\n",
            " Epoch 9, training [200 / 200]\n",
            " Epoch 9, validation [50 / 50]\n",
            "Epochs: 9, train loss=0.138216, val acc= 0.836591\n",
            " Epoch 10, training [200 / 200]\n",
            " Epoch 10, validation [50 / 50]\n",
            "Epochs: 10, train loss=0.136676, val acc= 0.839040\n",
            " Epoch 11, training [200 / 200]\n",
            " Epoch 11, validation [50 / 50]\n",
            "Epochs: 11, train loss=0.135555, val acc= 0.834112\n",
            " Epoch 12, training [200 / 200]\n",
            " Epoch 12, validation [50 / 50]\n",
            "Epochs: 12, train loss=0.134337, val acc= 0.820729\n",
            " Epoch 13, training [200 / 200]\n",
            " Epoch 13, validation [50 / 50]\n",
            "Epochs: 13, train loss=0.133046, val acc= 0.839701\n",
            " Epoch 14, training [200 / 200]\n",
            " Epoch 14, validation [50 / 50]\n",
            "Epochs: 14, train loss=0.132328, val acc= 0.825688\n",
            " Epoch 15, training [200 / 200]\n",
            " Epoch 15, validation [50 / 50]\n",
            "Epochs: 15, train loss=0.131006, val acc= 0.824024\n",
            " Epoch 16, training [200 / 200]\n",
            " Epoch 16, validation [50 / 50]\n",
            "Epochs: 16, train loss=0.130371, val acc= 0.825262\n",
            " Epoch 17, training [200 / 200]\n",
            " Epoch 17, validation [50 / 50]\n",
            "Epochs: 17, train loss=0.129859, val acc= 0.818387\n",
            " Epoch 18, training [200 / 200]\n",
            " Epoch 18, validation [50 / 50]\n",
            "Epochs: 18, train loss=0.129188, val acc= 0.811540\n",
            " Epoch 19, training [200 / 200]\n",
            " Epoch 19, validation [50 / 50]\n",
            "Epochs: 19, train loss=0.129002, val acc= 0.818459\n",
            " Epoch 20, training [200 / 200]\n",
            " Epoch 20, validation [50 / 50]\n",
            "Epochs: 20, train loss=0.128033, val acc= 0.806248\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3aRxWiUOnk3",
        "colab_type": "code",
        "outputId": "09a76e56-246e-45ab-e1e5-57152a732847",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        }
      },
      "source": [
        "history"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train_loss': [0.35719351679086686,\n",
              "  0.19515861615538596,\n",
              "  0.17161544673144818,\n",
              "  0.1599555177241564,\n",
              "  0.15168562401086091,\n",
              "  0.1474785041064024,\n",
              "  0.14394167464226484,\n",
              "  0.14121955297887326,\n",
              "  0.1382161197066307,\n",
              "  0.13667573098093272,\n",
              "  0.13555529214441775,\n",
              "  0.13433661215007306,\n",
              "  0.13304641470313072,\n",
              "  0.13232807476073505,\n",
              "  0.13100639309734105,\n",
              "  0.13037075463682413,\n",
              "  0.12985925544053317,\n",
              "  0.12918778263032438,\n",
              "  0.12900206699967384,\n",
              "  0.12803275350481272],\n",
              " 'val_acc': [0.6761421319796954,\n",
              "  0.8038955289951305,\n",
              "  0.8288907996560618,\n",
              "  0.8412631578947368,\n",
              "  0.8460264900662251,\n",
              "  0.8463123197363,\n",
              "  0.8488042156465343,\n",
              "  0.8393574297188755,\n",
              "  0.836591086786552,\n",
              "  0.8390397481306573,\n",
              "  0.8341121495327102,\n",
              "  0.8207293666026871,\n",
              "  0.8397006695549428,\n",
              "  0.8256880733944955,\n",
              "  0.8240244835501147,\n",
              "  0.8252615265401007,\n",
              "  0.8183873398643556,\n",
              "  0.811539902585238,\n",
              "  0.8184591914569032,\n",
              "  0.8062476757158795]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TOut8AFOr5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}